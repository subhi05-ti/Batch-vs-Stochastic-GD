# Batch-vs-Stochastic-GD
 BGD uses the entire training dataset to compute the gradient, while SGD uses only a single data point at a time.
