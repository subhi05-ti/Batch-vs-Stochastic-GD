# Batch-vs-Stochastic-GD
Batch Gradient Descent and Stochastic Gradient Descent are two fundamental variations of the Gradient Descent optimization algorithm, used to minimize a cost function in machine learning models.
 BGD uses the entire training dataset to compute the gradient, while SGD uses only a single data point at a time.
